# ğŸ¤Ÿ Sign Language to Text and Speech Conversion

A machine learning-powered project that converts American Sign Language (ASL) gestures into real-time text and speech using computer vision and gesture recognition.

## ğŸ“Œ Overview
This project recognizes ASL alphabets, digits, and special gestures (space, full stop) from a webcam feed and translates them into text, which is then spoken aloud using text-to-speech (TTS). It's designed to help improve accessibility and bridge communication gaps.

![asl](https://github.com/user-attachments/assets/0d7e21b0-fe97-4649-93ef-dacecf0aae21)

## ğŸ¯ Features

- ğŸ“· **Real-time Sign Detection** using a webcam
- ğŸ” **ASL Gesture Recognition**: Supports A-Z, 0â€“9, Space, and Full Stop
- ğŸ”Š **Text-to-Speech (TTS)** using `pyttsx3`
- ğŸ§  **Pre-trained Model** using a Random Forest Classifier
- ğŸ‘¨â€ğŸ’» **Custom Dataset Creation & Training Scripts** included

## ğŸ› ï¸ Tech Stack

- **Languages**: Python
- **Libraries**: OpenCV, MediaPipe, scikit-learn, pyttsx3, Tkinter
- **ML Model**: Random Forest Classifier

## ğŸš€ Getting Started

### 1. Clone the Repository
git clone https://github.com/Praveen1425/Sign-Language-Translation.git

cd Sign-Language-Translation
### 2. Install Requirements
pip install -r requirements.txt
### 3. Run the Application
Ensure your webcam is connected
python main.py

### ğŸ”® Future Improvements
Improve accuracy with deep learning models (e.g., CNNs)
Build a mobile/web version
### ğŸ™Œ Acknowledgments
Inspired by open-source contributions in the accessibility and ML community.


## ğŸ“‚ Project Structure

```plaintext
â”œâ”€â”€ collectImgs.py         # Capture gesture images
â”œâ”€â”€ createDataset.py       # Process and extract features
â”œâ”€â”€ trainClassifier.py     # Train Random Forest model
â”œâ”€â”€ main.py                # Real-time recognition and speech
â”œâ”€â”€ model.p                # Pre-trained model
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


